{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C4_W3_Colab_BERT_Loss_Model.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yuytuIllsv1"
      },
      "source": [
        "# Assignment 3, Part 1: BERT Loss Model \n",
        "\n",
        "Welcome to the part 1 of testing the models for this week's assignment. We will perform decoding using the BERT Loss model. In this notebook we'll use an input, mask (hide) random word(s) in it and see how well we get the \"Target\" answer(s). \n",
        "\n",
        "## IMPORTANT\n",
        "\n",
        "- As you cannot save the changes you make to this colab, you have to make a copy of this notebook in your own drive and run that. You can do so by going to `File -> Save a copy in Drive`. Close this colab and open the copy which you have made in your own drive.\n",
        "\n",
        "- Go to this [google drive folder](https://drive.google.com/drive/folders/1v3-aEfvs8ZMNRvEl36O7Hd5_pzszqTSz?usp=sharing) named `NLP_C4_W3_Colabs`. In the folder, next to its name use the drop down menu to select `\"Add shortcut to Drive\" -> \"My Drive\" and then press ADD SHORTCUT`. This should add a shortcut to the folder `NLP_C4_W3_Colabs` within your own google drive. Please make sure this happens, as you'll be reading the data for this notebook from this folder.\n",
        "\n",
        "- Make sure your runtime is GPU (_not_ CPU or TPU). And if it is an option, make sure you are using _Python 3_. You can select these settings by going to `Runtime -> Change runtime type -> Select the above mentioned settings and then press SAVE`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Db6LQW5cMSgx"
      },
      "source": [
        "**Note: Restarting the runtime maybe required**.\n",
        "\n",
        "Colab will tell you if the restarting is necessary -- you can do this from the:\n",
        "\n",
        "Runtime > Restart Runtime\n",
        "\n",
        "option in the dropdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qibASJKuDxpm"
      },
      "source": [
        "## Outline\n",
        "\n",
        "- [Part 0: Downloading and loading dependencies](#0)\n",
        "- [Part 1: Mounting your drive for data accessibility](#1)\n",
        "- [Part 2: Getting things ready](#2)\n",
        "- [Part 3: Part 3: BERT Loss](#3)\n",
        "    - [3.1 Decoding](#3.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysxogfC1M158"
      },
      "source": [
        "<a name='0'></a>\n",
        "# Part 0: Downloading and loading dependencies\n",
        "\n",
        "Uncomment the code cell below and run it to download some dependencies that you will need. You need to download them once every time you open the colab. You can ignore the `kfac` error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BNZzCg0xv3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e61579f5-7257-4279-c086-0bb1ef2268b5"
      },
      "source": [
        "!pip -q install trax"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 637 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 29.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 47.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install t5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NXUJhbCbumiQ",
        "outputId": "20ab16d8-e42b-4079-c734-b421cdfaed9b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting t5\n",
            "  Downloading t5-0.9.3-py3-none-any.whl (153 kB)\n",
            "\u001b[K     |████████████████████████████████| 153 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from t5) (2.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from t5) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from t5) (1.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from t5) (1.3.5)\n",
            "Collecting sacrebleu\n",
            "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
            "\u001b[K     |████████████████████████████████| 90 kB 7.4 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 47.2 MB/s \n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Collecting seqio\n",
            "  Downloading seqio-0.0.7-py3-none-any.whl (286 kB)\n",
            "\u001b[K     |████████████████████████████████| 286 kB 34.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from t5) (1.0.0)\n",
            "Requirement already satisfied: gin-config in /usr/local/lib/python3.7/dist-packages (from t5) (0.5.0)\n",
            "Requirement already satisfied: six>=1.14 in /usr/local/lib/python3.7/dist-packages (from t5) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from t5) (1.21.5)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.7/dist-packages (from t5) (0.5.3)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.7/dist-packages (from t5) (2.8.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from t5) (1.4.1)\n",
            "Collecting transformers>=2.7.0\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 32.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from t5) (3.2.5)\n",
            "Collecting tfds-nightly\n",
            "  Downloading tfds_nightly-4.5.2.dev202202240044-py3-none-any.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 32.8 MB/s \n",
            "\u001b[?25hCollecting mesh-tensorflow[transformer]>=0.1.13\n",
            "  Downloading mesh_tensorflow-0.1.19-py3-none-any.whl (366 kB)\n",
            "\u001b[K     |████████████████████████████████| 366 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (0.16.0)\n",
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.7/dist-packages (from mesh-tensorflow[transformer]>=0.1.13->t5) (4.0.1)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 30.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (21.3)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 43.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (4.11.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=2.7.0->t5) (2019.12.20)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 42.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers>=2.7.0->t5) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=2.7.0->t5) (3.0.7)\n",
            "Requirement already satisfied: pytz>=2015.7 in /usr/local/lib/python3.7/dist-packages (from babel->t5) (2018.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=2.7.0->t5) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->t5) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers>=2.7.0->t5) (3.0.4)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.4.0-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->t5) (0.8.9)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=2.7.0->t5) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->t5) (3.1.0)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (2.3)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (1.6.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (0.3.4)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (0.1.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (3.17.3)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (1.1.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (5.4.0)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (21.4.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-metadata->tensorflow-datasets->mesh-tensorflow[transformer]>=0.1.13->t5) (1.54.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->t5) (0.12.0)\n",
            "Requirement already satisfied: tensorflow<2.9,>=2.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->t5) (2.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.13.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.6.3)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.1.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (57.4.0)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (13.0.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (2.8.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.24.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.5.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.43.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.5.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.35.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow<2.9,>=2.8.0->tensorflow-text->t5) (3.2.0)\n",
            "Collecting etils[epath-no-tf]\n",
            "  Downloading etils-0.4.0-py3-none-any.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Installing collected packages: etils, toml, pyyaml, tokenizers, tfds-nightly, sentencepiece, sacremoses, portalocker, mesh-tensorflow, huggingface-hub, colorama, transformers, seqio, sacrebleu, rouge-score, t5\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed colorama-0.4.4 etils-0.4.0 huggingface-hub-0.4.0 mesh-tensorflow-0.1.19 portalocker-2.4.0 pyyaml-6.0 rouge-score-0.0.4 sacrebleu-2.0.0 sacremoses-0.0.47 sentencepiece-0.1.96 seqio-0.0.7 t5-0.9.3 tfds-nightly-4.5.2.dev202202240044 tokenizers-0.11.5 toml-0.10.2 transformers-4.16.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow_datasets"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDhi6qLQMHzs"
      },
      "source": [
        "import pickle\n",
        "import string\n",
        "import ast\n",
        "import numpy as np\n",
        "import trax \n",
        "from trax.supervised import decoding\n",
        "import textwrap \n",
        "# Will come handy later.\n",
        "wrapper = textwrap.TextWrapper(width=70)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cwr7LoXwQUW5"
      },
      "source": [
        "<a name='1'></a>\n",
        "# Part 1: Mounting your drive for data accessibility\n",
        "\n",
        "Run the code cell below and follow the instructions to mount your drive. The data is the same as used in the coursera version of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7ZF7KiXzQEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "608c904b-75fe-4652-bc40-7ae1ad02268e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xqT_O_9Mx0l"
      },
      "source": [
        "path = \"/content/drive/My Drive/NLP_C4_W3_Colabs\""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otO5C5VwEJoU"
      },
      "source": [
        "<a name='2'></a>\n",
        "# Part 2: Getting things ready \n",
        "\n",
        "Run the code cell below to ready some functions which will later help us in decoding. The code and the functions are the same as the ones you previsouly ran on the coursera version of the assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSmcZ_t8HBq6"
      },
      "source": [
        "example_jsons = list(map(ast.literal_eval, open(path + \"/data/data.txt\")))\n",
        "\n",
        "natural_language_texts = [example_json['text'] for example_json in example_jsons]\n",
        "\n",
        "PAD, EOS, UNK = 0, 1, 2\n",
        " \n",
        "def detokenize(np_array):\n",
        "  return trax.data.detokenize(\n",
        "      np_array,\n",
        "      vocab_type = 'sentencepiece',\n",
        "      vocab_file = 'sentencepiece.model',\n",
        "      vocab_dir = path + \"/models/\")\n",
        " \n",
        "def tokenize(s):\n",
        "  # The trax.data.tokenize function operates on streams,\n",
        "  # that's why we have to create 1-element stream with iter\n",
        "  # and later retrieve the result with next.\n",
        "  return next(trax.data.tokenize(\n",
        "      iter([s]),\n",
        "      vocab_type = 'sentencepiece',\n",
        "      vocab_file = 'sentencepiece.model',\n",
        "      vocab_dir = path + \"/models/\"))\n",
        " \n",
        "vocab_size = trax.data.vocab_size(\n",
        "    vocab_type = 'sentencepiece',\n",
        "    vocab_file = 'sentencepiece.model',\n",
        "    vocab_dir = path + \"/models/\")\n",
        "\n",
        "def get_sentinels(vocab_size):\n",
        "    sentinels = {}\n",
        "\n",
        "    for i, char in enumerate(reversed(string.ascii_letters), 1):\n",
        "\n",
        "        decoded_text = detokenize([vocab_size - i]) \n",
        "        \n",
        "        # Sentinels, ex: <Z> - <a>\n",
        "        sentinels[decoded_text] = f'<{char}>'\n",
        "        \n",
        "    return sentinels\n",
        "\n",
        "sentinels = get_sentinels(vocab_size)   \n",
        "\n",
        "\n",
        "def pretty_decode(encoded_str_list, sentinels=sentinels):\n",
        "    # If already a string, just do the replacements.\n",
        "    if isinstance(encoded_str_list, (str, bytes)):\n",
        "        for token, char in sentinels.items():\n",
        "            encoded_str_list = encoded_str_list.replace(token, char)\n",
        "        return encoded_str_list\n",
        "  \n",
        "    # We need to decode and then prettyfy it.\n",
        "    return pretty_decode(detokenize(encoded_str_list))\n",
        "\n",
        "\n",
        "inputs_targets_pairs = []\n",
        "\n",
        "# here you are reading already computed input/target pairs from a file\n",
        "with open (path + \"/data/inputs_targets_pairs_file.txt\", 'rb') as fp:\n",
        "    inputs_targets_pairs = pickle.load(fp)  \n",
        "\n",
        "\n",
        "def display_input_target_pairs(inputs_targets_pairs):\n",
        "    for i, inp_tgt_pair in enumerate(inputs_targets_pairs, 1):\n",
        "      inps, tgts = inp_tgt_pair\n",
        "      inps, tgts = pretty_decode(inps), pretty_decode(tgts)\n",
        "      print(f'[{i}]\\n'\n",
        "            f'inputs:\\n{wrapper.fill(text=inps)}\\n\\n'\n",
        "            f'targets:\\n{wrapper.fill(text=tgts)}\\n\\n\\n\\n')      "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHLYA6N7Izzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df76ee82-a21f-4aff-f175-270a1f06079c"
      },
      "source": [
        "display_input_target_pairs(inputs_targets_pairs)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1]\n",
            "inputs:\n",
            "Beginners BBQ <Z> Taking <Y> in Missoula! <X> want to get better <W>\n",
            "making delicious <V>? You will have the opportunity, put this on <U>\n",
            "calendar now <T> Thursday, September 22nd<S> World Class BBQ Champion,\n",
            "Tony Balay from Lonestar Smoke Rangers. He<R> be <Q> a beginner<P>\n",
            "class for everyone <O> wants to<N> better <M> their <L> skills. He\n",
            "will teach you <K> you need to know <J> compete in  <I> KCBS BBQ\n",
            "competition, including techniques, recipes,<H>s, meat selection<G>\n",
            "trimming, plus smoker <F> information. The cost to be in the class is\n",
            "$35 per person<E> for spectator<D> is free. Included in the cost will\n",
            "be either a t-shirt or apron and you will<C> tasting samples of each\n",
            "meat that <B>.\n",
            "\n",
            "targets:\n",
            "<Z> Class <Y> Place <X> Do you <W> at <V> BBQ <U> your <T>.<S> join<R>\n",
            "will <Q> teaching<P> level <O> who<N> get <M> with <L> culinary <K>\n",
            "everything <J> to <I>a<H> timeline<G> and <F> and fire<E>, and<D>s\n",
            "it<C> be <B> is prepared\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[2]\n",
            "inputs:\n",
            "Discussion <Z> ' <Y> X Lion (10.7)' started by  <X>xboi87, Jan <W>\n",
            "2012. I've got a 500gb <V> drive and  <U> 240gb SSD <T> When trying to\n",
            "restore using disk utility i'm given the<S>Not enough space<R> disk\n",
            "___ <Q> to restore<P> But I shouldn't have to do that!!! <O> or<N>s\n",
            "before resorting to the <M>? Use <L> Copy <K>ner to copy one drive to\n",
            "the other. <J>'ve done this several times <I> larger<H>D<G> smaller\n",
            "SSD and I wound up with a bootable SSD drive. One step you have to\n",
            "remember not to skip is <F> use Disk Utility to partition the SSD as\n",
            "GUID partition scheme HFS+<E> doing the<D>ne<C> If it came Apple\n",
            "Partition Scheme, even if you let CCC do the  <B>e <A> the resulting\n",
            "<z> won' <y> be bootable<x> CCC<w> works in \"file mode\" and it can\n",
            "easily copy a larger drive (that's mostly empty) onto<v> drive.<u> you\n",
            "tell CCC to clone <t>a drive <s> did NOT boot from, it can work in<r>\n",
            "copy mode where the destination drive must be the same size or larger\n",
            "than<q> drive you are cloning from<p>if I recall). I've actually done\n",
            "<o> somehow <n> Disk Utility several times <m>booting from <l>a\n",
            "different drive (<k> even the dvd) so not running <j> utility from the\n",
            "drive your cloning) and had it<i> just fine from larger to smaller\n",
            "bootable clone. Definitely<h> drive <g>ning to first, as bootable\n",
            "Apple <f>.. Thanks for pointing this out<e> only experience using DU\n",
            "to go larger to smaller was when I was trying to make <d>a <c> install\n",
            "<b> I was unable to restore InstallESD.d <a>g Théâtre Keep 4 GB USB\n",
            "stick but of course the reason that wouldndürftigt fit isutti was\n",
            "slightly more than 4  Carolyn of data.\n",
            "\n",
            "targets:\n",
            "<Z> in <Y>Mac OS <X>a <W> 20, <V> internal <U>a <T>.<S> error \"<R> on\n",
            "<Q>_<P>\" <O> Any ideas<N> workaround <M> above <L> Carbon <K> Clo <J>\n",
            "I <I> going from<H> HD<G> to <F> to<E> before<D> clo<C>. <B>clon <A>,\n",
            "<z> drive <y>t<x>.<w> usually<v> a smaller<u> If <t>  <s> you<r>\n",
            "block<q> the<p> ( <o> this <n> on <m> ( <l> <k>or <j> disk<i> work<h>\n",
            "format the<g>clo <f> etc<e>. My <d>  <c> Lion <b> stick and <a>m\n",
            "Théâtre toKeepadürftig'utti there CarolynGB\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[3]\n",
            "inputs:\n",
            "Fo <Z> plaid ly <Y> and <X>dex shortall with metallic slinky insets.\n",
            "Attached metallic elastic <W> with O-ring. <V>band <U>. Great hip hop\n",
            "<T> dance costume. Made in the USA.\n",
            "\n",
            "targets:\n",
            "<Z>il <Y>cra <X> span <W> belt <V> Head <U> included <T> or jazz\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[4]\n",
            "inputs:\n",
            "How many back <Z>s per day for <Y> site? Discussion in 'Black Hat SEO\n",
            "<X> by Omoplat <W>, Dec 3, 2010. 1) <V> a newly created site, what's\n",
            "the max # back <U>s per day I should do to be <T>? 2) how long do I\n",
            "have to let my site age before I can start<S> blinks? I did about <R>\n",
            "profiles every 24 hours <Q> 10 days for one of<P> sites which had a\n",
            "brand new domain. There is three backlinks for <O> of these<N> profile\n",
            "so thats 18 000 backlinks <M> 24 hours and nothing happened in terms\n",
            "of being penalized or sandboxed. This is now <L> 3 <K> ago and the\n",
            "site is ranking on first page for a lot of my targeted keywords <J>\n",
            "build more you can in starting but <I> manual<H> and not spammy\n",
            "type<G> manual + relevant to the <F>.. then after 1 month you can make\n",
            "a big<E>.. Wow, dude, you built 18k backlinks a day on a brand new<D>?\n",
            "How quickly<C> rank up? What <B> of competition/searches did those\n",
            "keywords have?\n",
            "\n",
            "targets:\n",
            "<Z>link <Y> new <X>' started <W>a <V> for <U>link <T> safe<S> making\n",
            "more<R>6000 forum <Q> for<P> my <O> every<N> forum <M> every <L> maybe\n",
            "<K> months <J>. <I> do<H> submission<G> means <F> post<E> blast<D>\n",
            "site<C> did you <B> kind\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[5]\n",
            "inputs:\n",
            "The Denver Board of Education opened the 2017-18 school year with an\n",
            "update on projects that include new construction, upgrades, heat\n",
            "mitigation and quality <Z>. We are excited <Y> students will be the\n",
            "<X> of a four year, <W>72 million <V> Obligation Bond. Since the\n",
            "passage of <U> bond, <T> construction team has worked to<S> the\n",
            "projects over the four-year term of<R> bond. Denver <Q> on Tuesday\n",
            "approved<P> and mill funding measures for students in <O> Public\n",
            "Schools, agreeing to invest $5<N> million in bond funding to build and\n",
            "improve schools and $5 <M> million in <L> dollars to support proven\n",
            "initiatives, such as early literacy. Denver voters say yes to bond and\n",
            "mill levy funding support <K> DPS students and schools. Click to learn\n",
            "<J> about the details <I> voter-approved bond measure. Denver voters\n",
            "on Nov. 8 approved bond and mill funding measures for DPS students and\n",
            "schools. Learn<H> about what’s included<G> the mill levy measure.\n",
            "\n",
            "targets:\n",
            "<Z> learning environments <Y> that Denver <X> beneficiaries <W> $5 <V>\n",
            "General <U> the <T> our<S> schedule<R> the <Q> voters<P> bond <O>\n",
            "Denver<N>72 <M>6.6 <L> operating <K> for <J> more <I> of the<H>\n",
            "more<G> in\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkfUvyjtEu6J"
      },
      "source": [
        "<a name='3'></a>\n",
        "# Part 3: BERT Loss\n",
        "\n",
        "We will not train the encoder which you have built in the assignment (coursera version). Training it could easily cost you a few days depending on which GPUs/TPUs you are using. Very few people train the full transformer from scratch. Instead, what the majority of people do, they load in a pretrained model, and they fine tune it on a specific task. That is exactly what you are about to do. Let's start by initializing and then loading in the model. \n",
        "\n",
        "Initialize the model from the saved checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsqi-dzzxv4e"
      },
      "source": [
        "# Initializing the model\n",
        "model = trax.models.Transformer(\n",
        "    d_ff = 4096,\n",
        "    d_model = 1024,\n",
        "    max_len = 2048,\n",
        "    n_heads = 16,\n",
        "    dropout = 0.1,\n",
        "    input_vocab_size = 32000,\n",
        "    n_encoder_layers = 24,\n",
        "    n_decoder_layers = 24,\n",
        "    mode='predict')  # Change to 'eval' for slow decoding."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOB1C131xv4i"
      },
      "source": [
        "# Now load in the model\n",
        "# this takes about 1 minute\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)  # Needed in predict mode.\n",
        "model.init_from_file(path + '/models/model.pkl.gz',\n",
        "                     weights_only=True, input_signature=(shape11, shape11))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Wy3pr4ZfzA_"
      },
      "source": [
        "# Uncomment to see the transformer's structure.\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuTyft5EBQK6"
      },
      "source": [
        "<a name='3.1'></a>\n",
        "### 3.1 Decoding\n",
        "\n",
        "Now you will use one of the `inputs_targets_pairs` for input and as target. Next you will use the `pretty_decode` to output the input and target. The code to perform all of this has been provided below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJ8s_xZ1QtkI"
      },
      "source": [
        "# # using the 3rd example\n",
        "c4_input = inputs_targets_pairs[2][0]\n",
        "c4_target = inputs_targets_pairs[2][1]\n",
        "\n",
        "# using the 1st example\n",
        "# c4_input = inputs_targets_pairs[0][0]\n",
        "# c4_target = inputs_targets_pairs[0][1]\n",
        "\n",
        "print('pretty_decoded input: \\n\\n', pretty_decode(c4_input))\n",
        "print('\\npretty_decoded target: \\n\\n', pretty_decode(c4_target))\n",
        "print('\\nc4_input:\\n\\n', c4_input)\n",
        "print('\\nc4_target:\\n\\n', c4_target)\n",
        "print(len(c4_target))\n",
        "print(len(pretty_decode(c4_target)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OD9EchfPFlAf"
      },
      "source": [
        "Run the cell below to decode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HwIdimiN0k2"
      },
      "source": [
        "# Faster decoding: (still - maybe lower max_length to 20 for speed)\n",
        "# Temperature is a parameter for sampling.\n",
        "#   # * 0.0: same as argmax, always pick the most probable token\n",
        "#   # * 1.0: sampling from the distribution (can sometimes say random things)\n",
        "#   # * values inbetween can trade off diversity and quality, try it out!\n",
        "output = decoding.autoregressive_sample(model, inputs=np.array(c4_input)[None, :],\n",
        "                                        temperature=0.0, max_length=50)\n",
        "print(wrapper.fill(pretty_decode(output[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n1MG7zNJZdh"
      },
      "source": [
        "### Note: As you can see the RAM is almost full, it is because the model and the decoding is memory heavy. Running it the second time might give you an answer that makes no sense, or repetitive words. If that happens restart the runtime (see how to at the start of the notebook) and run all the cells again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PRbChEc8dqe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}